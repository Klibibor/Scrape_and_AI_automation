const puppeteer = require('puppeteer-core');

// function for scraping
// uses puppeteer
// connects to existing Chrome instance via remote debugging port
// scrapes currently open page
async function scrapeWithBrowserConnect() {
    console.log('ğŸ”— PUPPETEER BROWSER CONNECT SCRAPER');
    console.log('=====================================');
    console.log('ğŸ“‹ Instrukcije:');
    console.log('1. Pokreni Chrome sa: chrome.exe --remote-debugging-port=9222');
    console.log('2. Otvori Å¾eljenu stranicu u browser-u');
    console.log('3. Pokreni ovaj script');
    console.log();
    // variable to hold browser instance
    let browser;
    try {
        // log connecting message
        console.log('ğŸ”— connecting to Chrome on port 9222...');

        // Connecting to existing Chrome browser
        browser = await puppeteer.connect({
            browserURL: 'http://127.0.0.1:9222',
            defaultViewport: null
        });
        // log it
        console.log('âœ… Successfully connected to Chrome!');

        // Get all open tabs/pages
        const pages = await browser.pages();
        console.log(`ğŸ“„ Found ${pages.length} open tabs`);

        // Look for Upwork tab first, otherwise use the first tab
        let page = pages[0];
        let upworkPage = null;

        for (let i = 0; i < pages.length; i++) {
            const pageUrl = pages[i].url();
            const pageTitle = await pages[i].title().catch(() => '');
            console.log(`ğŸ“„ Tab ${i}: ${pageTitle} - ${pageUrl}`);

            if (pageUrl.includes('upwork.com') || pageTitle.toLowerCase().includes('upwork')) {
                upworkPage = pages[i];
                console.log(`ğŸ¯ Found Upwork tab: ${pageTitle}`);
                break;
            }
        }

        // Use Upwork page if found, otherwise use first page
        if (upworkPage) {
            page = upworkPage;
            console.log(`âœ… Using Upwork page`);
        } else {
            console.log(`âš ï¸ No Upwork page found, using current active page`);
        }

        // Check current page URL and title
        const currentUrl = page.url();
        const pageTitle = await page.title();
        console.log(`ğŸ“„ Current page: ${pageTitle}`);
        console.log(`ğŸ”— URL: ${currentUrl}`);

        // Simulate short human actions to allow the page to fully load
        console.log('ğŸ­ Simulating short human actions...');
        await page.evaluate(() => {
            window.scrollTo(0, 100);
        });
        await new Promise(resolve => setTimeout(resolve, 1000));

        await page.evaluate(() => {
            window.scrollTo(0, 0);
        });
        await new Promise(resolve => setTimeout(resolve, 500));

        // inside variable content get full HTML of the page
        const content = await page.content();
        console.log(`ğŸ“Š HTML length: ${content.length} characters`);

        // Save HTML to data folder
        const fs = require('fs');
        const path = require('path');
        const timestamp = new Date().toISOString().replace(/[:.]/g, '-');
        const filename = `browser_scrape_${timestamp}.html`;
        const dataDir = path.join(__dirname, '..', 'data', 'data_raw');
        const fullPath = path.join(dataDir, filename);

        // Create data_raw folder if it doesn't exist
        if (!fs.existsSync(dataDir)) {
            fs.mkdirSync(dataDir, { recursive: true });
        }

        fs.writeFileSync(fullPath, content);
        console.log(`ğŸ’¾ HTML saved to: data/data_raw/${filename}`);

        // File saved successfully - parser will be called separately by n8n workflow
        console.log('ğŸ’¾ HTML saved - parser will run separately');

        // log content analysis
        const hasContent = content.length > 1000;
        console.log(`ğŸ” Contains content: ${hasContent ? 'YES' : 'NO'}`);

        if (hasContent) {
            console.log('âœ… SUCCESS! Page scraped!');

            // Estimated word count (rough estimate)
            const wordCount = content.split(/\s+/).length;
            console.log(`ğŸ“Š Estimated ${wordCount} words in HTML`);

            return true;
        } else {
            console.log('âš ï¸ Page is empty or failed to load properly.');
            return false;
        }

    } catch (error) {
        console.log(`âŒ Error: ${error.message}`);
        return false;
    } finally {
        // dont close the browser, just disconnect
        if (browser) {
            console.log('ğŸ”— Browser remains open');
        }
    }
}
// Main function to run the scraper
async function main() {
    try {
        const success = await scrapeWithBrowserConnect();
        // if success log success message
        if (success) {
            console.log('\nğŸ‰ SUCCESS! Scraper worked!');
            process.exit(0);
            // else log failed message
        } else {
            console.log('\nğŸ’” Failed');
            process.exit(1);
        }
    } catch (error) {
        console.log(`ğŸ’¥ CRITICAL ERROR: ${error.message}`);
        process.exit(1);
    }
}

if (require.main === module) {
    main();
}